# Set environment and import data

```{r}
rm(list=ls())
install.packages("rjags")
install.packages("ggplot2")
install.packages("patchwork")
library(reshape2)
library("ggplot2")
library(patchwork)
```

reading data

```{r}
wine=readRDS(file="./data/wine.Rda")
```

# Functions
```{r}
source("utils_functions.R")
```

# Data analysis

Plotting the distribution of quality wines.

```{r}
colnames(wine)
```

```{r}
# settings
alpha_density = .2

bars_color = "purple"
edges_color = "purple"
  

# plots
p1 <- ggplot(data=wine, aes(x=fixed.acidity)) + geom_histogram(color=edges_color, fill=bars_color)
p2 <- ggplot(data=wine, aes(x=volatile.acidity)) + geom_histogram(color=edges_color, fill=bars_color)
p3 <- ggplot(data=wine, aes(x=citric.acid)) + geom_histogram(color=edges_color, fill=bars_color)
p4 <- ggplot(data=wine, aes(x=residual.sugar)) + geom_histogram(color=edges_color, fill=bars_color)
p5 <- ggplot(data=wine, aes(x=chlorides)) + geom_histogram(color=edges_color, fill=bars_color)
p6 <- ggplot(data=wine, aes(x=free.sulfur.dioxide)) + geom_histogram(color=edges_color, fill=bars_color)
p7 <- ggplot(data=wine, aes(x=total.sulfur.dioxide)) + geom_histogram(color=edges_color, fill=bars_color)
p8 <- ggplot(data=wine, aes(x=density)) + geom_histogram(color=edges_color, fill=bars_color)
p9 <- ggplot(data=wine, aes(x=pH)) + geom_histogram(color=edges_color, fill=bars_color)
p10 <- ggplot(data=wine, aes(x=sulphates)) + geom_histogram(color=edges_color, fill=bars_color)
p11 <- ggplot(data=wine, aes(x=alcohol)) + geom_histogram(color=edges_color, fill=bars_color)
p12 <- ggplot(data=wine, aes(x=quality)) + geom_histogram(color=edges_color, fill=bars_color)

#layout
p1 + p2 + p3 + p4 + p5 + p6 + p7 + p8 + p9 + p10 + p11 + p12 + plot_layout(ncol = 3)
```

## Correlation analysis

We may think using both Pearson and Spearman rank correlation coefficient

Plotting correlation matrix using Pearson index.

```{r}
plot_correlation(wine, "Pearson\n correlation")
```

When performing predictions of quality we should care about the correlation among the covariates. If two covariates are highly correlated we may encounter strange results, so we should also consider to keep only low correlated covariates.

Find highly correlated covariates (absolute value of Pearson index greater than or equal to 0.5) here.

```{r}
melted_wine = melt(cor(wine))
correlated_covariates = find_competitive_covariates(melted_wine, 0.5, "quality")
correlated_covariates
```

Here I am looking for the covariates that seems to be more significant according to the Pearson coefficient.

```{r}
correlation_with_quality = tail(melted_wine, n=12)
correlation_with_quality["value"] = abs(correlation_with_quality["value"])
correlation_with_quality = correlation_with_quality[order(correlation_with_quality$value, decreasing = TRUE),]
correlation_with_quality
```

Considerations: the parameters are sorted by importance. We may expect to keep the parameters with the highest correlation index with quality while performing feature selection.

From the results above we can perform a prior feature selection. Given a pair of highly correlated covariates I want to discard the less significant.

Note: here I am not doing a feature selection with the goal of finding the most significant features (this will be done with the right choice of the prior) but I am just discarding in a greedy way features that may lead to problems when training the model.

In this case we may consider to remove density and free.sulfur.dioxide

```{r}
to_remove <- c("density", "free.sulfur.dioxide")
filtered_wine <- wine[, ! names(wine) %in% to_remove, drop=F]
filtered_wine
```

```{r}
plot_correlation(filtered_wine, "Pearson\n correlation")
```

## Managing outliers

```{r}
boxplot(wine, las=3,main="Covariates boxplot",cex.axis=0.75)
```

Find outliers in the original dataset.

```{r}
wine_m = remove_outliers(wine)
print(nrow(wine_m))
```

Find outliers in the filtered dataset.

```{r}
boxplot(filtered_wine, las=3,main="Covariates boxplot",cex.axis=0.75)
```

```{r}
wine_f_m = remove_outliers(filtered_wine)
nrow(wine_f_m)
```


simple regression( Gaussian / poisson ???? likelihood):
  - Lasso prior 
  - spikes & slab
make some analysis with both datasets (with/without outliers)

classification (categorical distribution likelihood):
  - Lasso prior ~ vlad
  - spikes & slab ~ joy

sensitive analysis 
compare posterior distributions of the betas.




