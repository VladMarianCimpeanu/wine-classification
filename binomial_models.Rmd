# Binomial regression

```{r}
rm(list=ls())
library(bayesplot)
library("ggplot2")
library(patchwork)
library(rjags)
library(coda)
source("utils_functions.R")
```

```{r}
wine = readRDS(file="./data/wine.Rda")
df <- remove_outliers(wine)
```

Bring the data to the right format.

```{r}
Y = as.vector(df$quality)
X = as.matrix(df[,!names(df) %in% c("quality"), drop=F])
```

Writing the model.

```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ dnorm(0, 0.01)
  for (i in 1: P){
    beta[i] ~ dnorm(0, 0.01)
  }
}")
```

```{r}
N <- dim(X)[1]  # number of observations
P <- dim(X)[2]

data <-list(Y=Y, X=X, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 100000
thin <-100
```

```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
# if we want to perform a larger burn in with not adaptation.
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta")

```

```{r}
# Sampling (this may take a while)
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```

```{r}
save(output, file="chains/binomial_model.dat")
```

```{r}
load("chains/binomial_model.dat")
```

```{r}
traceplots <- ggplot_all_traces(output, 11)
traceplots
#ggsave("pictures/trace_binomial_gaussian.png", traceplots)
```

```{r}
densities <- ggplot_all_densities(output, 11)
densities
#ggsave("pictures/density_binomial_gaussian.png", densities)
```

```{r}
p <- ggplot_all_autocorr(output, 11, thinning = 100)
p 
#ggsave("pictures/autocor_binomial_gaussian.png", p)
```

2, 4, 6, 11 inside the CI interval 95%

```{r}
names(df)[2]
names(df)[4]
names(df)[6]
names(df)[11]
```

```{r}
beta_0 <- as.matrix(output[,12])
betas <- as.matrix(output[, 1:11])
```

```{r}
id  = 250
Xnew <- X[id,]

n_iter <- length(beta_0)
y_pred <- numeric(n_iter)
for (i in 1: n_iter){
  y_pred[i] <-10 * pnorm(beta_0[i] + sum(Xnew * betas[i,]), mean=0, sd=1)
}

plt <- ggplot(data=data.frame(y_pred), aes(y_pred)) +
  geom_histogram(bins=150, alpha=1, color=light_blue_ex, fill=light_blue_ex) + 
  geom_vline(mapping=aes(xintercept=Y[id]), col=red_ex, lwd=0.9) + 
  ylab("predicted probability") + xlab("") +
  scale_x_continuous(breaks=c(0:10), limits=c(0,10))

plt
```

```{r}
# We consider only 100 subjects
error = 0; subN = length(Y)
missclassified = 0
for(i in 1:subN){
  Xnew = X[i,]
  for(g in 1:n_iter) # loop over the iterations
    y_pred[g] = pnorm(beta_0[g] + sum(Xnew*betas[g,]), mean = 0, sd = 1 )
  eps = round(abs(Y[i] - 10 * mean(y_pred) ), digits = 0 )
  error = error + eps 
  if (eps > 1){
    missclassified = missclassified + 1
  }
}
missclassified
ratio = missclassified / subN
ratio
error/ subN
```
