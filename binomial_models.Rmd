# Binomial regression

```{r}
rm(list=ls())
install.packages("rjags")
install.packages("ggplot2")
install.packages("patchwork")
install.packages("bayesplot")
library(bayesplot)
library("ggplot2")
library(patchwork)
library(rjags)
library(coda)
source("utils_functions.R")
```

```{r}
wine = readRDS(file="./data/wine.Rda")
df <- remove_outliers(wine)
```


Bring the data to the right format.
```{r}
Y = as.vector(df$quality)
X = as.matrix(df[,!names(df) %in% c("quality"), drop=F])
```

Writing the model.

```{r}
string_binomial <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dbin(phi(beta0 + X[i,] %*% beta[]), 10)
  }
  # Prior
  beta0 ~ dnorm(0, 0.01)
  for (i in 1: P){
    beta[i] ~ dnorm(0, 0.01)
  }
}")
```


```{r}
N <- dim(X)[1]  # number of observations
P <- dim(X)[2]

data <-list(Y=Y, X=X, N=N, P=P)

burn     <- 1000
n.iter.update   <- 5000
n.chains <- 2
# Number of iterations & thinning
nit <- 100000
thin <-100
```

```{r}
binomial_model <- jags.model(string_binomial,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
# if we want to perform a larger burn in with not adaptation.
cat("  Updating...\n")
update(binomial_model, n.iter=n.iter.update)

# Posterior parameters JAGS has to track
param <- c("beta0", "beta")

```

```{r}
# Sampling (this may take a while)
cat("  Sampling...\n")
output <- coda.samples(model = binomial_model,
                       variable.names = param,
                       n.iter = nit,
                       thin = thin)
```
```{r}
save(output, file="chains/binomial_model.dat")
```

```{r}
load("chains/binomial_model.dat")
```


```{r}
plot(output)
print(names(df))
```

```{r}
autocorr.plot(output[1])
```

```{r}

get_beta_names <- function(start, end){
  betas <- c()
  for (i in start: end){
    betas <- append(betas, paste("beta[", as.character(i), "]", sep=""))
  }
  return(betas)
}
```


```{r}
color_scheme_set("brightblue")
mcmc_areas(
  output,            
  pars = "beta[3]",     # make a plot for the mu parameter
  prob = 0.95)
```


```{r}
color_scheme_set("purple")
mcmc_trace_highlight(output, pars = get_beta_names(3,3), highlight = 2)
```


```{r}
summary(output)
```
2, 4, 6, 11

```{r}
names(df)[2]
names(df)[4]
names(df)[6]
names(df)[11]
```
```{r}
beta_0 <- as.matrix(output[,12])
betas <- as.matrix(output[, 1:11])
```

```{r}
id  = 250
Xnew <- X[id,]

n_iter <- length(beta_0)
y_pred <- numeric(n_iter)
for (i in 1: n_iter){
  y_pred[i] <-10 * pnorm(beta_0[i] + sum(Xnew * betas[i,]), mean=0, sd=1)
}

plt <- ggplot(data=data.frame(y_pred), aes(y_pred)) +
  geom_histogram(bins=100, alpha=0.6) + 
  geom_vline(mapping=aes(xintercept=Y[id]), col="red", lwd=1.5) + 
  theme_minimal() + theme(legend.position = "none") + 
  ylab("predicted probability") + xlab("") +
  xlim(0, 10) + geom_vline(mapping=aes(xintercept=1), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=2), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=3), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=4), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=5), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=6), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=7), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=8), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=9), col=3, lwd=0.5) +
  geom_vline(mapping=aes(xintercept=10), col=3, lwd=0.5)

plt
```


```{r}
# We consider only 100 subjects
error = 0; subN = length(Y)
missclassified = 0
for(i in 1:subN){
  Xnew = X[i,]
  for(g in 1:n_iter) # loop over the iterations
    y_pred[g] = pnorm(beta_0[g] + sum(Xnew*betas[g,]), mean = 0, sd = 1 )
  eps = round(abs(Y[i] - 10 * mean(y_pred) ), digits = 0 )
  error = error + eps 
  if (eps > 1){
    missclassified = missclassified + 1
  }
}
missclassified
ratio = missclassified / subN
ratio
errBL
```





