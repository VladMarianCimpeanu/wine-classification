---
editor_options: 
  markdown: 
    wrap: sentence
---

```{r}
rm(list=ls())
library("ggplot2")
library(patchwork)
library(rjags)
library(coda)
```

# Intro

In this notebook we try to implement a new model for the data.
Likelihood of the quality is categorical and each parameter is got using the
softmax link function.
We start with a simple non informative Gaussian prior for the regressors.

Importing utils functions

```{r}
source("utils_functions.R")
```

Importing dataset and removing outliers

```{r}
wine = readRDS(file="./data/wine.Rda")
wine = remove_outliers(wine)
```

Drop competitive covariates

```{r}
df <- wine[, !names(wine) %in% c("density", "total.sulfur.dioxide", "chlorides", "residual.sugar")]
df
```

Drop low correlated covariates.
free.sulfur.dioxide sulphates, citric.acid

```{r}
df <- df[, !names(df) %in% c("free.sulfur.dioxide",	"sulphates", "citric.acid")]
df
```

Take a portion of dataset of training.

```{r}
indexes = sample(1:nrow(df), size=0.1 * nrow(df))
train <- df[indexes,]
test <- df[-indexes,]
df <- train
```

Plotting training quality histogram to be sure the distribution of quality is still the same.

```{r}
ggplot(data=wine[indexes,],) + geom_histogram(aes(x=quality), bins=10, center=0, binwidth = 1, color=red_ex, fill=red_ex) + xlim(c(0, 10))
```

Bring the data to the right format.

```{r}
Y = as.vector(df$quality)
X = as.matrix(df[,!names(df) %in% c("quality"), drop=F])
```

Writing the model.

```{r}
model_string <- textConnection("model{
  # Likelihood
  for (i in 1:N){
    Y[i] ~ dcat(pi[i, 1:C])
    for (c in 1:C) {
      exp_z[i,c] <- exp(z[i, c])                  # softmax function
      pi[i, c]    <- exp_z[i, c]/sum(exp_z[i, ])   # softmax function
      z[i, c]    <- beta[c, ] %*% X[i, ]          # linear model
    }
  }
  # Prior
  for (c in 1: C){
    for (k in 1: K){
      beta[c, k] ~ dnorm(0, 0.01)       # Gaussian prior
    }
  }
}")
```

```{r}
N <- dim(X)[1]  # number of observations
K <- dim(X)[2]  # number of covariates
C <- 10         # number of categories


data <-list(Y=Y, X=X, N=N, C=C, K=K)

burn     <- 500
n.iter   <- 5000
thin     <- 5
n.chains <- 2
```

```{r}
model <- jags.model(model_string,data = data, n.chains=n.chains,quiet=FALSE)
```

```{r}
update(model, burn, n.iter=n.iter)
```

```{r}
samples <- coda.samples(model, variable.names=c("beta"), thin=thin, n.iter=n.iter)
```
We have tried to run a model with thinning = 100 and even more data (50% of the
dataset), but still we were not able to reach better results, so we keep 
thinning to 5, so to have more samples.
```{r}
load("chains/categorical_gaussian.dat")
```

```{r}
betas <- c()
for (i in 1: 4){
  for(j in 1:10)
    betas <- append(betas, paste("beta.", as.character(j), ".", as.character(i), ".", sep=""))
}
thin(samples)

# SAVE THE PLOTS
for (i in 1:40){
  print(i)
  x <-ggplot_traceplot(samples, betas[i])
  ggsave(paste("pictures/softmax/traces/trace_beta", as.character(i), ".png", sep=""), plot=x)
  x <-ggplot_density_MC(samples, betas[i])
  ggsave(paste("pictures/softmax/densities/density_beta", as.character(i), ".png", sep=""), plot=x)
}


```

Prediction with softmax using the test set.
Categorical distribution have a list of parameters $p_i$ such that:

```{=tex}
\begin{align*}
  $p_i =  \frac{e^{\beta_i X_i}}{\sum_je^{\beta_jX_i}}$
\end{align*}
```


```{r}
indexes_test = sample(1:nrow(test), size=0.1 * nrow(test))
test_thinned <- test[+indexes_test, ]
Y_test = as.vector(test_thinned$quality)
X_test = as.matrix(test[, -c(ncol(test)), drop=F])
length(Y_test)
ggplot(data=wine[indexes_test,],) + geom_histogram(aes(x=quality), bins=10, center=0, binwidth = 1, color=red_ex, fill=red_ex) + xlim(c(0, 10))
```


```{r}
betas <- as.matrix(samples[, 1:40])

id = 1
x_new = X_test[id, ]
n_iter = 1000

```

```{r}
#computing exponentials
exp_s <- matrix(ncol=10, nrow=n_iter)
# here I am storing the samples for each p_i of the categorical distribution
p_s <- matrix(ncol=10, nrow = n_iter)
for (j in 1:n_iter){
  # sample from the posterior distribution of beta
  for (i in 1:10){
    #getting the betas for the i-th category for the j-th sample 
    beta_cat = betas[j, c(i, 10 + i, 20 + i, 30 + i)]
    exp_s[j, i] <- exp(sum(x_new * beta_cat))       
  }
  for (i in 1:10){
    p_s[j, i] <- exp_s[j, i] / sum(exp_s[j, ])
  }
}
```

```{r}

# saving plots of prediction
p_s = data.frame(p_s)
which.max(colMeans(p_s))
for(i in 1:10){
  param <- paste("X", as.character(i), sep="")
  main_col = light_blue_ex
  if(i == Y_test[id]){
    main_col = red_ex
  }
  z <-ggplot(data=p_s) + 
    geom_density(aes_string(x=param), alpha=1, color=main_col, fill=main_col)
  ggsave(paste("pictures/softmax/predictions/category_p", as.character(i), ".png", sep=""), plot=z)
}

```

Even though we are using a classification approach, actually this is still a 
regression task, since labels are quantitative and can be somehow measured and 
ordered.
The only difference with the classical regression problem is that the label domain
is restricted to a specific interval (0 -> 10).
For this reason, we think the best way to measure the error is using MSE metric
instead of the percentage of miss classified points-
```{r}
n_iter = 1000
eps_s = numeric(length(Y_test))
pred_vs_true = matrix(nrow=length(Y_test), ncol=3)
for (sample_test in 1: length(Y_test)){
  # select new sample
  x <- X_test[sample_test,]
  # storing all the exponential of the regressions needed to compute the 
  # softmax function
  exp_s <- matrix(ncol=10, nrow=n_iter)
  # here I am storing the samples for each p_i of the categorical distribution
  p_s <- matrix(ncol=10, nrow = n_iter)
  for (j in 1:n_iter){
    # sample from the posterior distribution of beta
    for (i in 1:10){
      #getting the betas for the i-th category for the j-th sample 
      beta_cat = betas[j, c(i, 10 + i, 20 + i, 30 + i)]
      exp_s[j, i] <- exp(sum(x * beta_cat))       
    }
    # compute link function 
    for (i in 1:10){
      p_s[j, i] <- exp_s[j, i] / sum(exp_s[j, ])
    }
  }
  # classify the wine by assign it to the category with the highest posterior mean
  y_pred = which.max(colMeans(p_s))
  # compute the error
  pred_vs_true[sample_test, 1] = sample_test
  pred_vs_true[sample_test, 2] = y_pred
  pred_vs_true[sample_test, 3] = Y_test[sample_test]
  eps_s[sample_test] = (y_pred - Y_test[sample_test]) ** 2
}
# compute MSE
mse_error <- sum((eps_s ** 0.5)) / length(Y_test) 
mse_error
```

Plotting results on the test set.

```{r}
results <-data.frame(
  "sample"=pred_vs_true[100:200,1],
  "prediction"=pred_vs_true[100:200, 2],
  "true_value"=pred_vs_true[100:200, 3]
  )


plt_results <-ggplot(data = results, mapping = aes(x = sample )) +
    geom_hline(aes(yintercept = 0)) +
    geom_segment(mapping = aes(xend =sample, y=true_value, yend = 0), size=1.5, color=red_ex, alpha=0.4) + 
    geom_segment(mapping = aes(xend =sample, y=prediction, yend = 0), size=1.5, color=light_blue_ex, alpha=0.4) + 
    scale_y_continuous(name="values", limits=c(0, 10))

plt_results
#ggsave("pictures/softmax/softmax_prediction_results.png", plot=plt_results)

```










